{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.61 (from langchain-openai)\n",
      "  Using cached langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Using cached openai-1.82.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting langsmith<0.4,>=0.1.126 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (24.2)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pydantic>=2.7.4 (from langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests<3,>=2 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Using cached langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
      "Using cached langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached openai-1.82.0-py3-none-any.whl (720 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-macosx_15_0_arm64.whl (133 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.5/633.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, orjson, jsonpointer, jiter, idna, h11, distro, charset-normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, jsonpatch, httpcore, anyio, tiktoken, requests-toolbelt, pydantic, httpx, openai, langsmith, langchain-core, langchain-openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31/31\u001b[0m [langchain-openai][langchain-core]lt]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.4.26 charset-normalizer-3.4.2 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.61 langchain-openai-0.3.18 langsmith-0.3.42 openai-1.82.0 orjson-3.10.18 pydantic-2.11.5 pydantic-core-2.33.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.9.0 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.1 urllib3-2.4.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x127f1f2f0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1303705c0> root_client=<openai.OpenAI object at 0x1252b49e0> root_async_client=<openai.AsyncOpenAI object at 0x127f1f470> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems that exhibit a degree of agency, meaning they possess the ability to act autonomously, make decisions, and pursue goals with minimal human intervention. Unlike traditional AI systems that perform specific, predefined tasks based on explicit programming, agentic AI systems can perceive their environment, set objectives, make decisions, and adapt their behavior based on feedback and changing circumstances.\\n\\n### Key Characteristics of Agentic AI:\\n\\n1. **Autonomy**: Agentic AI can operate independently without continuous human oversight. It can initiate actions, make choices, and adjust its behavior based on internal and external factors.\\n\\n2. **Goal-Oriented Behavior**: These AI systems are designed to achieve specific objectives. They can prioritize tasks, allocate resources, and adjust strategies to meet their goals effectively.\\n\\n3. **Perception and Adaptation**: Agentic AI can sense and interpret changes in its environment using various sensors or data inputs. It can adapt its actions based on new information, learning from experiences to improve future performance.\\n\\n4. **Decision-Making Capabilities**: These systems incorporate decision-making processes that allow them to evaluate options, assess risks, and choose actions that align with their objectives.\\n\\n5. **Learning and Improvement**: Agentic AI often employs machine learning techniques to enhance its performance over time. It can learn from data, identify patterns, and refine its strategies to become more efficient and effective.\\n\\n### Applications of Agentic AI:\\n\\n- **Autonomous Vehicles**: Self-driving cars must perceive their surroundings, make real-time driving decisions, and navigate safely without constant human control.\\n  \\n- **Robotics**: Advanced robots in manufacturing, healthcare, or service industries can perform complex tasks, adapt to new situations, and collaborate with humans.\\n  \\n- **Virtual Assistants**: AI-driven personal assistants can manage schedules, answer queries, and proactively provide information based on user preferences and behaviors.\\n  \\n- **Strategic Business Systems**: Agentic AI can optimize supply chains, manage inventories, and make strategic decisions to improve business operations.\\n\\n### Implications and Considerations:\\n\\n- **Ethics and Accountability**: As AI systems gain more autonomy, determining responsibility for their actions becomes more complex. Ensuring ethical behavior and establishing accountability frameworks is crucial.\\n  \\n- **Safety and Control**: Developing safeguards to prevent unintended consequences is essential, especially in high-stakes applications like healthcare or transportation.\\n  \\n- **Transparency**: Understanding how agentic AI systems make decisions helps build trust and allows for better oversight, ensuring that their actions align with human values and societal norms.\\n\\n### Conclusion\\n\\nAgentic AI represents a significant advancement in artificial intelligence, moving beyond simple task automation to systems capable of independent thought and action. While this evolution offers numerous benefits, it also presents challenges that require careful consideration to ensure that such technologies are developed and deployed responsibly.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 901, 'prompt_tokens': 13, 'total_tokens': 914, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_7989eaacf6', 'id': 'chatcmpl-Bast9ZXjFBHjuaKMmPsw8av7v4nfp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e46d49c5-dd6b-426b-bd74-b24a4902eabb-0' usage_metadata={'input_tokens': 13, 'output_tokens': 901, 'total_tokens': 914, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems that exhibit a degree of agency, meaning they possess the ability to act autonomously, make decisions, and pursue goals with minimal human intervention. Unlike traditional AI systems that perform specific, predefined tasks based on explicit programming, agentic AI systems can perceive their environment, set objectives, make decisions, and adapt their behavior based on feedback and changing circumstances.\n",
      "\n",
      "### Key Characteristics of Agentic AI:\n",
      "\n",
      "1. **Autonomy**: Agentic AI can operate independently without continuous human oversight. It can initiate actions, make choices, and adjust its behavior based on internal and external factors.\n",
      "\n",
      "2. **Goal-Oriented Behavior**: These AI systems are designed to achieve specific objectives. They can prioritize tasks, allocate resources, and adjust strategies to meet their goals effectively.\n",
      "\n",
      "3. **Perception and Adaptation**: Agentic AI can sense and interpret changes in its environment using various sensors or data inputs. It can adapt its actions based on new information, learning from experiences to improve future performance.\n",
      "\n",
      "4. **Decision-Making Capabilities**: These systems incorporate decision-making processes that allow them to evaluate options, assess risks, and choose actions that align with their objectives.\n",
      "\n",
      "5. **Learning and Improvement**: Agentic AI often employs machine learning techniques to enhance its performance over time. It can learn from data, identify patterns, and refine its strategies to become more efficient and effective.\n",
      "\n",
      "### Applications of Agentic AI:\n",
      "\n",
      "- **Autonomous Vehicles**: Self-driving cars must perceive their surroundings, make real-time driving decisions, and navigate safely without constant human control.\n",
      "  \n",
      "- **Robotics**: Advanced robots in manufacturing, healthcare, or service industries can perform complex tasks, adapt to new situations, and collaborate with humans.\n",
      "  \n",
      "- **Virtual Assistants**: AI-driven personal assistants can manage schedules, answer queries, and proactively provide information based on user preferences and behaviors.\n",
      "  \n",
      "- **Strategic Business Systems**: Agentic AI can optimize supply chains, manage inventories, and make strategic decisions to improve business operations.\n",
      "\n",
      "### Implications and Considerations:\n",
      "\n",
      "- **Ethics and Accountability**: As AI systems gain more autonomy, determining responsibility for their actions becomes more complex. Ensuring ethical behavior and establishing accountability frameworks is crucial.\n",
      "  \n",
      "- **Safety and Control**: Developing safeguards to prevent unintended consequences is essential, especially in high-stakes applications like healthcare or transportation.\n",
      "  \n",
      "- **Transparency**: Understanding how agentic AI systems make decisions helps build trust and allows for better oversight, ensuring that their actions align with human values and societal norms.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Agentic AI represents a significant advancement in artificial intelligence, moving beyond simple task automation to systems capable of independent thought and action. While this evolution offers numerous benefits, it also presents challenges that require careful consideration to ensure that such technologies are developed and deployed responsibly.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-groq) (0.3.61)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user just introduced themselves as Muthu Selvam. I should start by greeting them back. Let me make sure to use their name to personalize the response. Since they provided their name, maybe they want a friendly acknowledgment. I should keep it simple and open-ended so they feel comfortable to ask anything or share more. Let me check if there\\'s any specific information I need to include. No, just a greeting. Alright, I\\'ll say something like, \"Hello, Muthu Selvam!\" and offer assistance. That should be good.\\n</think>\\n\\nHello, Muthu Selvam! Nice to meet you. How can I assist you today? 😊', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 20, 'total_tokens': 161, 'completion_time': 0.34752021, 'prompt_time': 0.003107804, 'queue_time': 0.108123105, 'total_time': 0.350628014}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--27030fed-4502-42a9-92d0-0886aa98d480-0', usage_metadata={'input_tokens': 20, 'output_tokens': 141, 'total_tokens': 161})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install langchain-groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Muthu Selvam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x131726e40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x131727350>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x131726e40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x131727350>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You bet! As an AI engineer, I know a thing or two about Langsmith.  \n",
      "\n",
      "**Langsmith** is a powerful open-source framework developed by **Cohere**, designed to simplify the process of building and deploying **large language models (LLMs)**.  \n",
      "\n",
      "Here's a breakdown of what makes Langsmith special:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** Langsmith emphasizes ease of use, even for those without extensive coding experience. It provides a web-based interface and intuitive tools for interacting with LLMs.\n",
      "\n",
      "* **Fine-Tuning Capabilities:** It allows you to fine-tune pre-trained LLMs on your own datasets, enabling you to customize models for specific tasks and domains.\n",
      "\n",
      "* **Model Management:** Langsmith streamlines the management of multiple LLMs, making it easy to track versions, compare performance, and switch between models.\n",
      "\n",
      "* **Experiment Tracking:** It helps you record and analyze the results of your LLM experiments, facilitating iterative development and model improvement.\n",
      "\n",
      "* **API Integration:** Langsmith offers APIs for seamless integration with other applications and workflows.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accelerated Development:**  Langsmith significantly reduces the time and effort required to build and deploy LLMs.\n",
      "\n",
      "* **Increased Accessibility:** Its user-friendly nature makes LLMs more accessible to a wider range of users, including researchers, developers, and businesses.\n",
      "\n",
      "* **Enhanced Customization:** Fine-tuning capabilities empower users to create specialized LLMs tailored to their unique needs.\n",
      "\n",
      "* **Improved Collaboration:** Langsmith's collaborative features facilitate teamwork and knowledge sharing within organizations.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Chatbots and Conversational AI**\n",
      "\n",
      "* **Text Generation and Summarization**\n",
      "\n",
      "* **Code Generation and Assistance**\n",
      "* **Data Analysis and Insights**\n",
      "* **Personalized Learning and Education**\n",
      "\n",
      "**Overall, Langsmith is a valuable tool for anyone looking to harness the power of LLMs in a more efficient, accessible, and customizable way.** \n",
      "\n",
      "Let me know if you have any more questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking about **Langsmith**, a fascinating tool in the world of AI! \n",
      "\n",
      "Think of Langsmith as a **powerful platform designed to simplify the process of building and deploying your own AI assistant applications**.  It's built on top of the Llama 2 large language model (LLM) from Meta, but its real strength lies in its user-friendliness and the features it offers. \n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "* **Open-Source and Accessible:** Langsmith is open-source, meaning its code is freely available for anyone to use, modify, and contribute to. This fosters collaboration and innovation within the AI community.\n",
      "\n",
      "* **Streamlined Development:** It provides a visual interface and intuitive tools that make it easier to fine-tune and customize Llama 2 for specific tasks. You don't need to be a deep learning expert to get started.\n",
      "* **Focus on Applications:** Langsmith emphasizes building practical applications rather than just experimenting with raw LLM capabilities. It helps you create AI assistants for tasks like:\n",
      "    * **Chatbots:** Build conversational agents that can interact with users naturally.\n",
      "    * **Text Generation:** Create stories, articles, summaries, or any type of text content.\n",
      "    * **Code Generation:** Assist developers by generating code snippets in different programming languages.\n",
      "    * **Data Analysis:**  Help analyze and summarize large datasets.\n",
      "* **Community Support:** As an open-source project, Langsmith benefits from a vibrant community of developers and users who share knowledge, provide support, and contribute to its growth.\n",
      "\n",
      "**In essence, Langsmith democratizes access to powerful AI technology, making it easier for individuals and organizations to build their own intelligent applications.**\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or anything else related to AI!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI assistants.', 'features': ['Modular design allows for easy customization and extension.', 'Supports multiple programming languages, including Python, JavaScript, and TypeScript.', 'Provides a variety of pre-trained models for natural language understanding, generation, and other tasks.', 'Offers tools for fine-tuning models on custom datasets.', 'Allows for integration with external APIs and services.', 'Cloud-based deployment options for scalable and reliable AI assistants.'], 'benefits': ['Faster development cycles for AI assistants.', 'Increased flexibility and control over AI models.', 'Lower costs compared to proprietary AI platforms.', 'Access to a thriving community of developers and users.', 'Open-source nature promotes transparency and collaboration.'], 'website': 'https://www.langsmith.com/'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n    <name>LangSmith</name>\\n    <description>LangSmith is an open-source platform designed to help developers build and deploy large language models (LLMs) more easily.</description>\\n    <features>\\n        <feature>Modular architecture for customization and extensibility</feature>\\n        <feature>Simplified model training and fine-tuning</feature>\\n        <feature>Integration with popular LLM frameworks</feature>\\n        <feature>Tools for model evaluation and monitoring</feature>\\n    </features>\\n    <website>https://github.com/langsmith-ai/langsmith</website>\\n</response>\\n```' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 195, 'total_tokens': 340, 'completion_time': 0.263636364, 'prompt_time': 0.007766457, 'queue_time': 0.067434676, 'total_time': 0.271402821}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--8155caf2-b31e-4bdb-957f-55dd2bec405a-0' usage_metadata={'input_tokens': 195, 'output_tokens': 145, 'total_tokens': 340}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain) (0.3.61)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain) (2.11.5)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: SQLAlchemy, langchain-text-splitters, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [langchain]/3\u001b[0m [langchain]text-splitters]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.41 langchain-0.3.25 langchain-text-splitters-0.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "content='<response><answer>LangChain is a framework for developing applications powered by large language models (LLMs). It provides tools and components to simplify the process of building with LLMs, including:\\n\\n* **Chain creation:**  Easily build sequences of calls to LLMs and other services.\\n* **Memory management:**  Give your applications a memory to store and recall past interactions.\\n* **Prompt engineering:**  Craft effective prompts to guide LLM responses.\\n* **Data integration:**  Connect LLMs with external data sources.\\n* **Agents:** Build autonomous agents that can interact with the world through APIs and other tools.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 39, 'total_tokens': 174, 'completion_time': 0.245454545, 'prompt_time': 0.002411333, 'queue_time': 0.091227392, 'total_time': 0.247865878}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--224e3f09-4436-4084-b326-373af8cdfb78-0' usage_metadata={'input_tokens': 39, 'output_tokens': 135, 'total_tokens': 174}\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "\n",
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Splash</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Da Vinci Code</movie>\n",
      "<movie>Toy Story (franchise)</movie>\n",
      "<movie>Bridge of Spies</movie>\n",
      "<movie>Sully</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9318a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_name': 'Apple iPhone 15', 'details': 'The Apple iPhone 15 is the latest flagship smartphone from Apple, featuring a stunning design, powerful performance, and advanced camera capabilities. It comes with a high-resolution display, fast processor, and the latest iOS software. The iPhone 15 also offers enhanced security features and a range of connectivity options.', 'price_usd': 1200}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the Pydantic model for product info\n",
    "class ProductInfo(BaseModel):\n",
    "    name: str = Field(description=\"Name of the product\")\n",
    "    details: str = Field(description=\"Short description of the product\")\n",
    "    price_usd: int = Field(description=\"Tentative price in USD\")\n",
    "\n",
    "# Use OpenAI LLM (or you can use any other LLM available as 'model')\n",
    "product_model = ChatOpenAI(temperature=0.3)\n",
    "\n",
    "# Output parser for the ProductInfo model\n",
    "product_parser = JsonOutputParser(pydantic_object=ProductInfo)\n",
    "\n",
    "# Prompt template with format instructions\n",
    "product_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. When asked about any product, respond in JSON with the product name, details, and a tentative price in USD (integer).\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "product_chain = product_prompt | product_model | product_parser\n",
    "\n",
    "# Example usage\n",
    "query = \"Tell me about the Apple iPhone 15.\"\n",
    "result = product_chain.invoke({\"input\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
